
开源的可用于LLM Pretrain数据集

| 数据集                                | 语言       | 大小            | 备注                                                                                                                                  | 地址                                                                                                                                                                        |
| ---------------------------------- | -------- | ------------- | ----------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| WuDaoCorpora                       | 中文       | 200G          | 北京智源研究院从100TB原始网页数据中清洗得出最终数据集，包含教育、科技等50+个行业数据标签，总共5TB，开源200G                                                                       | [https://data.baai.ac.cn/details/WuDaoCorporaText](https://data.baai.ac.cn/details/WuDaoCorporaText "https://data.baai.ac.cn/details/WuDaoCorporaText")                   |
| WanJuan1.0                         | 中英文      | 约1T，中文约500G   | 上海AI实验室从来自网页、百科、书籍、专利、教材、考题等不同来源的清洗后预训练语料组成，数据总量超过5亿个文档，数据大小超过1TB。                                                                  | [https://opendatalab.org.cn/OpenDataLab/WanJuan1\_dot\_0](https://opendatalab.org.cn/OpenDataLab/WanJuan1_dot_0 "https://opendatalab.org.cn/OpenDataLab/WanJuan1_dot_0")  |
| 蜜巢·花粉1.0                           | 中文       | 约240G         | 蜜度公司从公开可访问的中文互联网数据，领域包括新闻、政务等。通过关键词过滤、图片抽取、规则过滤、格式转换等一系列数据处理流程，最终清洗后的数据达7000余万条，同时包括100余万个图片链接。                                     | [https://opendatalab.org.cn/OpenDataLab/MiChao](https://opendatalab.org.cn/OpenDataLab/MiChao "https://opendatalab.org.cn/OpenDataLab/MiChao")                            |
| MNBVC                              | 中文       | 目前20T         | 中文数据开源之光！&#xA;MNBVC数据集包括新闻、作文、小说、书籍、杂志、论文、台词、帖子、wiki、古诗、歌词、商品介绍、笑话、糗事、聊天记录等一切形式的纯文本中文数据，数据均来源于互联网收集。                                | [https://github.com/esbatmop/MNBVC](https://github.com/esbatmop/MNBVC "https://github.com/esbatmop/MNBVC")                                                                |
| TigerBot                           | 中英文      | 中文约50G，英文约50G | Tiger基于 GPT3 的 pretrain 的数据分布，采集中文书籍，互联网，和百科类数据，并通过数据源质量分过滤和 tf-idf soft deduping，从 20TB 数据过滤到 2TB，保持语言和类目的比例，并在此基础上随机抽样 100G 数据开源。 | [https://github.com/TigerResearch/TigerBot#开源数据集](https://github.com/TigerResearch/TigerBot#开源数据集 "https://github.com/TigerResearch/TigerBot#开源数据集")                      |
| CLUECorpus2020                     | 中文       | 约100G         | 通过对Common Crawl的中文部分进行语料清洗，最终得到100GB的高质量中文预训练语料                                                                                     | [https://github.com/CLUEbenchmark/CLUECorpus2020/](https://github.com/CLUEbenchmark/CLUECorpus2020/ "https://github.com/CLUEbenchmark/CLUECorpus2020/")                   |
| FinCorpus                          | 中文       | 约60G          | 度小满开源的中文金融资讯数据集                                                                                                                     | [https://huggingface.co/datasets/Duxiaoman-DI/FinCorpus](https://huggingface.co/datasets/Duxiaoman-DI/FinCorpus "https://huggingface.co/datasets/Duxiaoman-DI/FinCorpus") |
| Chinese\_book\_dataset             | 中文       | 13.3万本        | 一个广泛搜集爬取的中文图书分类数据集。数据采集自各大电子书网站。                                                                                                    | [https://github.com/JiangYanting/Chinese\_book\_dataset](https://github.com/JiangYanting/Chinese_book_dataset "https://github.com/JiangYanting/Chinese_book_dataset")     |
| CulturaX                           | 多语言，主要英文 | 共27T，约1T中文    | 用于167种语言的大型语言模型的多语言数据集，数据集经过比较彻底的清理阶段                                                                                               | [https://huggingface.co/datasets/uonlp/CulturaX](https://huggingface.co/datasets/uonlp/CulturaX "https://huggingface.co/datasets/uonlp/CulturaX")                         |
| Bloom                              | 多语言，主要英文 | 共1.6T，约10G中文  | BLOOM是在ROOTS的语料上训练的，其是一个由498个Hugging Face数据集组成的语料。共计1.61TB的文本，包含46种自然语言和13种编程语言。                                                    | [https://huggingface.co/bigscience-data](https://huggingface.co/bigscience-data "https://huggingface.co/bigscience-data")                                                 |
| Common Crawl                       | 多语言，主要英文 | 每月更新          | Common Crawl 每个月都会发布一个快照，包含了随机搜索和采样的 URL 所获得的原始网页。                                                                                  | [https://commoncrawl.org/](https://commoncrawl.org/ "https://commoncrawl.org/")                                                                                           |
| Colossal Clean Crawled Corpus (C4) | 多语言，主要英文 | 最新版约17T       | 基于Common Crawl数据清洗得到的，最初被Google用来训练 T5 模型，最新版是2023年4月的3.1.0版本。                                                                      | [https://www.tensorflow.org/datasets/catalog/c4](https://www.tensorflow.org/datasets/catalog/c4 "https://www.tensorflow.org/datasets/catalog/c4")                         |
| The Pile                           | 主要英文     | 825G          | 由22个高质量数据集集合并进一步处理的预训练数据集                                                                                                           | [https://pile.eleuther.ai/](https://pile.eleuther.ai/ "https://pile.eleuther.ai/")                                                                                        |
| RedPajama                          | 主要英文     | 约5T           | 复刻llama的预训练数据集                                                                                                                      | [https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data "https://github.com/togethercomputer/RedPajama-Data")             |
| Wikipedia                          | 主要英文     | 更新中           | 维基百科的数据                                                                                                                             | [https://huggingface.co/datasets/wikipedia](https://huggingface.co/datasets/wikipedia "https://huggingface.co/datasets/wikipedia")                                        |
| WebText2                           | 主要英文     | 约65G          | 从Reddit提交的URL中抓取的文档构成                                                                                                               | [https://github.com/EleutherAI/openwebtext2](https://github.com/EleutherAI/openwebtext2 "https://github.com/EleutherAI/openwebtext2")                                     |
| BookCorpus                         | 英文       | 约3G           | 英文图书                                                                                                                                | [https://github.com/soskek/bookcorpus](https://github.com/soskek/bookcorpus "https://github.com/soskek/bookcorpus")                                                       |
| ArXiv                              | 英文       | 约170万篇        | 英文学术论文                                                                                                                              | [https://huggingface.co/datasets/arxiv\_dataset](https://huggingface.co/datasets/arxiv_dataset "https://huggingface.co/datasets/arxiv_dataset")                           |
| 几个数据集平台                            |          |               |                                                                                                                                     |                                                                                                                                                                           |
| CLUEDatasetSearch                  | 中文       | 多NLP任务数据集合    | 中英文NLP相关任务数据集的集合                                                                                                                    | [https://github.com/CLUEbenchmark/CLUEDatasetSearch](https://github.com/CLUEbenchmark/CLUEDatasetSearch "https://github.com/CLUEbenchmark/CLUEDatasetSearch")             |
| OpenDataLab                        | 中英文      | 多NLP任务数据集合    | 数据集平台                                                                                                                               | [https://opendatalab.org.cn/home](https://opendatalab.org.cn/home "https://opendatalab.org.cn/home")                                                                      |
| Huggingface datasets               | 多语言      | 多数据集          | 数据集平台                                                                                                                               | [https://huggingface.co/datasets](https://huggingface.co/datasets "https://huggingface.co/datasets")                                                                      |
| 千言数据集                              | 中文       | 多NLP任务数据集合    | 数据集平台                                                                                                                               | [https://www.luge.ai/#/](https://www.luge.ai/#/ "https://www.luge.ai/#/")                                                                                                 |
| 天池数据集                              | 中文       | 多NLP任务数据集合    | 数据集平台                                                                                                                               | [https://tianchi.aliyun.com/dataset/](https://tianchi.aliyun.com/dataset/ "https://tianchi.aliyun.com/dataset/")                                                          |
| kaggle                             | 主要英文     | 多NLP任务数据集合    | 数据集平台                                                                                                                               | [https://www.kaggle.com/](https://www.kaggle.com/ "https://www.kaggle.com/")                                                                                              |
| hyper                              | 中文       | 各种数据集         | 数据集平台                                                                                                                               | [https://hyper.ai/datasets](https://hyper.ai/datasets "https://hyper.ai/datasets")                                                                                        |
